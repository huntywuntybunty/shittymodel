import sqlite3
import numpy as np
import pandas as pd
from typing import Union, List, Dict, Optional, Any, Tuple
import math
from functools import lru_cache
import asyncio
import os
import json
import aiofiles
import logging
import hashlib
from requests import Session
from pybaseball import playerid_lookup
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential


def load_stuff_plus():
    """Load Stuff+ values from CSV"""
    try:
        df = pd.read_csv("data/fangraphs_stuff_plus.csv")
        return {row["Name"]: row["Stuff+"] for _, row in df.iterrows()}
    except Exception as e:
        print(f"⚠️ Could not load Stuff+ CSV: {e}")
        return {}


stuff_plus_lookup = load_stuff_plus()

with open("custom_lineups.json") as f:
    CUSTOM_LINEUPS = json.load(f)

# Configuration
REQUEST_TIMEOUT = 10
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
CACHE_TTL = timedelta(hours=6)

# Ensure cache directories exist
os.makedirs('cache', exist_ok=True)
os.makedirs('vegas_cache', exist_ok=True)

# Hardcoded pitcher IDs
PITCHER_IDS = {
    "Pablo López": 669272, "Pablo Lopez": 669272,
    "Zac Gallen": 666157, "Max Scherzer": 453286,
    "Tyler Glasnow": 607192, "Griffin Canning": 664037,
    "Zack Wheeler": 554430, "Nick Pivetta": 605483,
    "Framber Valdez": 664285, "Jack Leiter": 693029,
    "S. Schweilenbach": 686534, "Michael Soroka": 656775,
    "Paul Skenes": 699901, "Mick Abel": 680757,
    "Jackson Jobe": 695094, "Jose Berrios": 593576,
    "Luis L. Ortiz": 656302, "Andrew Abbott": 687128,
    "M. Liberatore": 668677, "Michael Wacha": 593580,
    "Shane Baz": 663556, "Zebby Matthews": 699872,
    "Freddy Peralta": 596146, "Jonathan Cannon": 694089,
    "Colin Rea": 595894, "Yusei Kikuchi": 657053,
    "Tony Gonsolin": 664137, "Jeffrey Springs": 621244,
    "Bryan Woo": 686436, "Michael King": 668731,
    "Chase Dollander": 700123, "Merrill Kelly": 572193,
    "David Peterson": 656305, "Max Fried": 594798
}

# Contextual Modifier Scraper
class ContextualScraper:
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))
    async def get_0_2_chase_rate(self, pitcher_id: int) -> float:
        """Scrapes 0-2 chase rate from Savant with fallback"""
        try:
            url = f"https://baseballsavant.mlb.com/savant-player/{pitcher_id}"
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=15) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    chase_div = soup.find("div", string=re.compile(r"0-2 Chase Rate"))
                    if chase_div:
                        chase_text = chase_div.find_next("span").text.strip('%')
                        return float(chase_text) / 100
        except Exception as e:
            logging.warning(f"Chase rate error for {pitcher_id}: {e}")
        return 0.32  # Fallback average

from tenacity import retry, stop_after_attempt, wait_exponential

class VegasLineFetcher:
    def __init__(self, api_key: str = None, cache_dir: str = 'vegas_cache'):
        self.api_key = api_key or os.getenv("ODDS_API_KEY", "79a51d8a41b909f35fa04b64393185de")
        self.cache_dir = cache_dir
        self.cache_expiry = timedelta(hours=6)
        os.makedirs(self.cache_dir, exist_ok=True)

    def _get_cache_path(self, pitcher_name: str) -> str:
        return os.path.join(self.cache_dir, f"{pitcher_name.lower().replace(' ', '_')}.json")

    def _load_from_cache(self, pitcher_name: str) -> Optional[Tuple[float, str]]:
        path = self._get_cache_path(pitcher_name)
        if os.path.exists(path):
            try:
                with open(path, 'r') as f:
                    data = json.load(f)
                    if datetime.fromisoformat(data['timestamp']) + self.cache_expiry > datetime.now():
                        return data['line'], data['source']
            except Exception:
                pass
        return None

    def _save_to_cache(self, pitcher_name: str, line: float, source: str):
        try:
            with open(self._get_cache_path(pitcher_name), 'w') as f:
                json.dump({
                    'line': line,
                    'source': source,
                    'timestamp': datetime.now().isoformat()
                }, f)
        except Exception:
            pass

    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def _fetch_from_odds_api(self, pitcher_name: str) -> Optional[Tuple[float, str]]:
        try:
            url = "https://api.the-odds-api.com/v4/sports/baseball_mlb/odds"
            params = {
                'apiKey': self.api_key,
                'regions': 'us',
                'markets': 'player_strikeouts',
                'oddsFormat': 'decimal'
            }
            res = requests.get(url, params=params, timeout=10)
            res.raise_for_status()
            data = res.json()
            for game in data:
                for bookmaker in game.get('bookmakers', []):
                    for market in bookmaker.get('markets', []):
                        if market['key'] == 'player_strikeouts':
                            for outcome in market.get('outcomes', []):
                                if pitcher_name.lower() in outcome['description'].lower():
                                    return float(outcome['point']), 'odds-api'
        except Exception:
            return None

    @retry(stop=stop_after_attempt(2))
    def _fetch_from_sportsbook_review(self, pitcher_name: str) -> Optional[Tuple[float, str]]:
        try:
            url = "https://www.sportsbookreview.com/betting-odds/mlb/"
            headers = {'User-Agent': USER_AGENT}
            res = requests.get(url, headers=headers, timeout=15)
            soup = BeautifulSoup(res.text, 'html.parser')
            props = soup.find('div', {'id': 'props'})
            if props:
                for row in props.find_all('tr'):
                    cols = row.find_all('td')
                    if len(cols) >= 2 and pitcher_name.lower() in cols[0].text.lower():
                        return float(cols[1].text.strip()), 'sportsbook-review'
        except Exception:
            return None

    def _get_manual_fallback(self, pitcher_name: str) -> Tuple[float, str]:
        manual_lines = {
            "pablo lópez": 6.5, "pablo lopez": 6.5,
            "zac gallen": 7.0, "max scherzer": 6.5,
            "tyler glasnow": 6.5
        }
        return manual_lines.get(pitcher_name.lower(), 6.5), 'manual-fallback'

    def get_vegas_line(self, pitcher_name: str) -> Tuple[float, str]:
        if cached := self._load_from_cache(pitcher_name):
            return cached
        for fetch_func in [self._fetch_from_odds_api, self._fetch_from_sportsbook_review]:
            if result := fetch_func(pitcher_name):
                line, source = result
                self._save_to_cache(pitcher_name, line, source)
                return line, source
        line, source = self._get_manual_fallback(pitcher_name)
        self._save_to_cache(pitcher_name, line, source)
        return line, source

# Initialize Vegas fetcher
vegas_fetcher = VegasLineFetcher()

# DB Setup
def init_tracking_db():
    conn = sqlite3.connect('k_tracking.db')
    conn.execute('''
        CREATE TABLE IF NOT EXISTS projections (
            date TEXT,
            pitcher TEXT,
            opponent TEXT,
            projected_mean REAL,
            vegas_line REAL,
            actual_ks INTEGER,
            PRIMARY KEY (date, pitcher)
        )
    ''')
    conn.commit()
    return conn

# Pitch weight map for vulnerability scoring
PITCH_WEIGHTS = {
    'SL': {'k_pct': 0.5, 'whiff_pct': 0.4, 'xwoba': 0.1},
    'FF': {'k_pct': 0.3, 'whiff_pct': 0.5, 'xwoba': 0.2},
    'CU': {'k_pct': 0.4, 'whiff_pct': 0.5, 'xwoba': 0.1},
}

VEGAS_BIAS = 0.92
LEAGUE_AVG_K = 6.5  # Fallback baseline
LEAGUE_AVG = {'k_pct': 0.22, 'whiff_pct': 0.28, 'xwoba': 0.320}
STD_DEV = {'k_pct': 0.06, 'whiff_pct': 0.08, 'xwoba': 0.040}

# Load team metrics
def load_team_trends():
    try:
        lhp = pd.read_csv("team_trends_lhp.csv").set_index("team").to_dict(orient="index")
        rhp = pd.read_csv("team_trends_rhp.csv").set_index("team").to_dict(orient="index")
        return lhp, rhp
    except Exception:
        return {}, {}

TEAM_TREND_LHP, TEAM_TREND_RHP = load_team_trends()

def load_contextual_team_metrics():
    try:
        df = pd.read_csv("team_chase_fp_swing.csv").set_index("team")
        return df.to_dict(orient="index")
    except Exception:
        return {}

TEAM_CONTEXTUAL = load_contextual_team_metrics()

# Vectorized batter vulnerability
def calculate_vulnerability(batters: List[Dict], pitch_type: str) -> np.ndarray:
    df = pd.DataFrame(batters)
    weights = PITCH_WEIGHTS.get(pitch_type, PITCH_WEIGHTS['SL'])
    z_k = ((df['k_pct'] - LEAGUE_AVG['k_pct']) / STD_DEV['k_pct']).clip(-2.5, 2.5)
    z_whiff = ((df['whiff_pct'] - LEAGUE_AVG['whiff_pct']) / STD_DEV['whiff_pct']).clip(-2.5, 2.5)
    z_xwoba = ((LEAGUE_AVG['xwoba'] - df['xwoba']) / STD_DEV['xwoba']).clip(-2.5, 2.5)
    return np.tanh(
        z_k * weights['k_pct'] +
        z_whiff * weights['whiff_pct'] +
        z_xwoba * weights['xwoba']
    ) * 2

# Dynamic modifier weights
def get_dynamic_weights(pitcher_hand: str, pitch_types: List[str]) -> List[float]:
    if 'SL' in pitch_types and 'FF' in pitch_types:
        return [0.5, 0.25, 0.15, 0.1]
    if 'CU' in pitch_types:
        return [0.35, 0.35, 0.2, 0.1]
    if pitcher_hand == 'L':
        return [0.45, 0.25, 0.2, 0.1]
    return [0.4, 0.3, 0.2, 0.1]


# --- STRIKEOUT PROJECTION CORE ---

def project_strikeouts(
    pitcher: str,
    pitcher_hand: str,
    base_ks: float,
    ks_logs: List[int],
    putaway_pitch: str,
    opponent_team: str,
    park: str,
    opponent_lineup: List[Dict]
) -> Dict:
    """Calculate strikeout projection with debug output"""
    
    print("\n=== BATTER STATS ===")
    for i, batter in enumerate(opponent_lineup[:5]):
        print(f"{i+1}. {batter.get('name', 'Unknown')}: "
              f"K%={batter['k_pct']:.1%}, Whiff%={batter['whiff_pct']:.1%}, Hand={batter['hand']}")

    batters = [preprocess_batter(b) for b in opponent_lineup]
    matchup_scores = calculate_vulnerability(batters, putaway_pitch)
    matchup_mod = 1 + (np.mean(matchup_scores) / 10)

    platoon_mod = np.mean([get_platoon_modifier(pitcher_hand, b['hand']) for b in batters])
    park_mod = get_park_modifier(park)
    team_mod = get_team_modifier(opponent_team, pitcher_hand)

    print("\n=== MODIFIERS ===")
    print(f"Matchup: {matchup_mod:.3f} (Avg score: {np.mean(matchup_scores):.3f})")
    print(f"Platoon: {platoon_mod:.3f}")
    print(f"Park: {park_mod:.3f}")
    print(f"Team: {team_mod:.3f}")

    weights = get_dynamic_weights(pitcher_hand, [putaway_pitch])
    total_mod = 1 + sum((
        (matchup_mod - 1) * weights[0],
        (platoon_mod - 1) * weights[1],
        (park_mod - 1) * weights[2],
        (team_mod - 1) * weights[3]
    ))
    total_mod = np.clip(total_mod, 0.85, 1.15)
    print(f"Total Modifier: {total_mod:.3f}")

    vegas_line, _ = vegas_fetcher.get_vegas_line(pitcher)
    vegas_effect = 1 / (1 + math.exp(-(vegas_line - 6.5)))
    adjusted_mean = base_ks * total_mod * (0.95 + 0.1 * vegas_effect)

    dispersion = get_pitcher_dispersion(ks_logs)
    samples = simulate_ks(adjusted_mean, dispersion)

    return {
        'pitcher': pitcher,
        'mean': round(adjusted_mean, 2),
        'distribution': {
            '25th': np.percentile(samples, 25),
            '50th': np.percentile(samples, 50),
            '75th': np.percentile(samples, 75),
            '95th': np.percentile(samples, 95)
        },
        'prob_over_5.5': round(np.mean(samples > 5.5) * 100, 2),
        'prob_over_6.5': round(np.mean(samples > 6.5) * 100, 2),
        'prob_over_7.5': round(np.mean(samples > 7.5) * 100, 2)
    }

# --- GAME LOGS ---

@lru_cache(maxsize=50)
def get_pitcher_game_logs(pitcher_id: int, games: int = 15) -> List[int]:
    try:
        url = f"https://www.baseball-reference.com/players/gl.fcgi?id={pitcher_id}&t=p&year=2024"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        k_logs = [
            int(row.find('td', {'data-stat': 'SO'}).text)
            for row in soup.select('table#pitching_gamelogs tbody tr')
            if row.find('td', {'data-stat': 'SO'})
        ]
        return k_logs[-games:] if k_logs else []
    except Exception as e:
        print(f"⚠️ Couldn't scrape game logs: {e}")
        return []

def calculate_ewma(k_logs: List[int], alpha: float = 0.3) -> float:
    if not k_logs:
        return LEAGUE_AVG_K
    weights = np.array([(1 - alpha)**i for i in range(len(k_logs))][::-1])
    return round(np.dot(k_logs, weights) / weights.sum(), 2)

# --- CONTEXTUAL MODIFIERS ---

stuff_plus_lookup = load_stuff_plus()

async def get_contextual_modifiers(pitcher_id: int, opponent_team: str, stuff_val: float) -> Dict[str, float]:
    try:
        scraper = ContextualScraper()
        chase_rate = await scraper.get_0_2_chase_rate(pitcher_id)
        return {
            '0_2_chase': 1 + (chase_rate - 0.32) * 2,
            'fp_aggression': 1.02,
            'stuff_plus': 1 + (stuff_val - 100) * 0.005
        }
    except Exception as e:
        logging.warning(f"Context mod error: {e}")
        return {'0_2_chase': 1.0, 'fp_aggression': 1.0, 'stuff_plus': 1.0}

# --- SUPPORTING UTILITIES ---

def get_pitcher_id(pitcher_name: str) -> Optional[int]:
    if pitcher_name in PITCHER_IDS:
        return PITCHER_IDS[pitcher_name]
    try:
        last, first = pitcher_name.split()[1], pitcher_name.split()[0]
        ids = playerid_lookup(last, first)
        return int(ids.iloc[0]['mlbam_id']) if not ids.empty else None
    except Exception:
        return None

def get_primary_putaway_pitch(pitcher_id: int) -> str:
    try:
        url = f"https://baseballsavant.mlb.com/savant-player/{pitcher_id}"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        rows = soup.select("table#pitchArsenal tr.pitch-arsenal-table")
        pitches = []
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 5:
                pitch = cols[0].text.strip()
                putaway = float(cols[-1].text.replace("%", "").strip())
                pitches.append((pitch, putaway))
        if not pitches:
            return "SL"
        return max(pitches, key=lambda x: x[1])[0][:2].upper()
    except Exception:
        return "SL"

def get_pitch_types(pitcher_id: int) -> List[str]:
    try:
        url = f"https://baseballsavant.mlb.com/savant-player/{pitcher_id}"
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')
        rows = soup.select("table#pitchArsenal tr.pitch-arsenal-table")[1:]
        types = set()
        for row in rows:
            cols = row.find_all("td")
            if len(cols) >= 2:
                types.add(cols[0].text.strip()[:2].upper())
        return list(types) if types else ["SL"]
    except Exception:
        print(f"⚠️ Pitch type error for ID {pitcher_id}")
        return ["SL"]

def get_pitcher_hand(pitcher_id: int) -> str:
    try:
        res = requests.get(f"https://statsapi.mlb.com/api/v1/people/{pitcher_id}", timeout=5)
        hand = res.json()['people'][0]['pitchHand']['code'].upper()
        if hand in ['L', 'R']:
            return hand
    except Exception:
        pass

    if pitcher_id == 699901:  # Paul Skenes
        return 'R'

    return 'R'  # Fallback

def preprocess_batter(batter: Dict) -> Dict:
    return {
        'k_pct': batter.get('k_pct', LEAGUE_AVG['k_pct']),
        'whiff_pct': batter.get('whiff_pct', LEAGUE_AVG['whiff_pct']),
        'xwoba': batter.get('xwoba', LEAGUE_AVG['xwoba']),
        'hand': batter.get('hand', 'R')
    }

def get_park_modifier(park: str) -> float:
    return {
        'Coors Field': 0.95,
        'Great American Ball Park': 1.05
    }.get(park, 1.0)

def get_team_modifier(team: str, pitcher_hand: str) -> float:
    trends = TEAM_TREND_LHP if pitcher_hand == 'L' else TEAM_TREND_RHP
    return 1 + (trends.get(team, {}).get('k_pct', 0) - LEAGUE_AVG['k_pct'])

def get_platoon_modifier(pitcher_hand: str, batter_hand: str) -> float:
    return 0.95 if pitcher_hand == batter_hand else 1.05

def get_pitcher_dispersion(ks_logs: List[int]) -> float:
    return 1.5 if len(ks_logs) < 5 else max(1.0, np.std(ks_logs) * 0.75)

def simulate_ks(mean: float, dispersion: float, n: int = 10000) -> np.ndarray:
    return np.random.normal(mean, dispersion, n).clip(0, 15)

TEAM_ABBREVIATIONS = {
    "Diamondbacks": "ARI", "Braves": "ATL", "Orioles": "BAL", "Red Sox": "BOS",
    "Cubs": "CHC", "White Sox": "CWS", "Reds": "CIN", "Guardians": "CLE",
    "Rockies": "COL", "Tigers": "DET", "Astros": "HOU", "Royals": "KC",
    "Angels": "LAA", "Dodgers": "LAD", "Marlins": "MIA", "Brewers": "MIL",
    "Twins": "MIN", "Yankees": "NYY", "Mets": "NYM", "Athletics": "OAK",
    "Phillies": "PHI", "Pirates": "PIT", "Padres": "SD", "Giants": "SF",
    "Mariners": "SEA", "Cardinals": "STL", "Rays": "TB", "Rangers": "TEX",
    "Blue Jays": "TOR", "Nationals": "WSH"
}

def validate_lineup(lineup: List[str]) -> bool:
    return (
        isinstance(lineup, list)
        and 8 <= len(lineup) <= 10
        and all(isinstance(name, str) and len(name.split()) >= 2 for name in lineup)
        and all(not name.lower().endswith("pitcher") for name in lineup)
    )

async def get_mlb_lineup(session: aiohttp.ClientSession, team_id: int) -> Optional[List[str]]:
    try:
        url = f"https://statsapi.mlb.com/api/v1/schedule?sportId=1&teamId={team_id}"
        async with session.get(url) as res:
            data = await res.json()
        game_id = data['dates'][0]['games'][0]['gamePk']
        url = f"https://statsapi.mlb.com/api/v1/game/{game_id}/linescore"
        async with session.get(url) as res:
            linescore = await res.json()
        lineup = [p['name']['boxscore'] for p in linescore['teams']['away']['players'].values()]
        return lineup if validate_lineup(lineup) else None
    except Exception as e:
        logging.debug(f"MLB API failed: {e}")
        return None

async def get_rotowire_lineup(session: aiohttp.ClientSession, team_abbrev: str) -> Optional[List[str]]:
    try:
        url = "https://www.rotowire.com/baseball/daily-lineups.php"
        async with session.get(url, headers={"User-Agent": USER_AGENT}) as res:
            soup = BeautifulSoup(await res.text(), 'html.parser')
            for box in soup.find_all("div", class_="lineup is-mlb"):
                if team_abbrev.upper() in box.find("div", class_="lineup__abbr").text.upper():
                    lineup = [p.find("a").text.strip() for p in box.find_all("div", class_="lineup__player")[:9] if p.find("a")]
                    return lineup if validate_lineup(lineup) else None
    except Exception as e:
        logging.debug(f"Rotowire lineup failed: {e}")
        return None

async def get_bref_lineup(session: aiohttp.ClientSession, team_abbrev: str) -> Optional[List[str]]:
    try:
        url = f"https://www.baseball-reference.com/teams/{team_abbrev}/lineups.shtml"
        async with session.get(url, headers={"User-Agent": USER_AGENT}) as res:
            soup = BeautifulSoup(await res.text(), 'html.parser')
        lineup = [row.select_one('td[data-stat="player"] a').text.strip()
                  for row in soup.select('table#lineups tbody tr:not(.thead)')][:9]
        return lineup if validate_lineup(lineup) else None
    except Exception as e:
        logging.debug(f"Baseball Reference lineup failed: {e}")
        return None

async def get_espn_lineup(session: aiohttp.ClientSession, team_abbrev: str) -> Optional[List[str]]:
    try:
        url = f"https://www.espn.com/mlb/team/lineup/_/name/{team_abbrev.lower()}"
        async with session.get(url, headers={"User-Agent": USER_AGENT}) as res:
            soup = BeautifulSoup(await res.text(), 'html.parser')
        lineup = [row.select_one('a.AnchorLink').text.strip()
                  for row in soup.select('div.Table__TR') if row.select_one('a.AnchorLink')][:9]
        return lineup if validate_lineup(lineup) else None
    except Exception as e:
        logging.debug(f"ESPN lineup failed: {e}")
        return None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))
async def get_fangraphs_stats(session: aiohttp.ClientSession, name: str) -> Optional[Dict]:
    try:
        url = f"https://www.fangraphs.com/players/{name.lower().replace(' ', '-')}/stats"
        async with session.get(url, headers={"User-Agent": USER_AGENT}) as res:
            if res.status != 200:
                return None
            html = await res.text()
            return await parse_fangraphs_stats(html)
    except Exception as e:
        logging.debug(f"FanGraphs stats failed for {name}: {e}")
        return None

async def parse_fangraphs_stats(html: str) -> Dict:
    try:
        soup = BeautifulSoup(html, "html.parser")
        script = soup.find("script", string=re.compile("var stats_json"))
        raw_json = re.search(r"var stats_json = (.*?);", script.string)
        data = json.loads(raw_json.group(1))
        latest = data['SeasonStats'][-1]
        return {
            "k_pct": float(latest.get('kPercent', 22)) / 100,
            "whiff_pct": 0.28,
            "xwoba": 0.320,
            "pa": int(latest.get('plateAppearances', 200)),
            "hand": latest.get('handedness', 'R')[0].upper()
        }
    except Exception as e:
        logging.warning(f"FanGraphs parse error: {e}")
        return {}

async def get_savant_stats(session: aiohttp.ClientSession, player_name: str) -> Tuple[float, float]:
    try:
        search_url = f"https://baseballsavant.mlb.com/search?searchTerm={player_name.replace(' ', '+')}"
        async with session.get(search_url, timeout=10, headers={"User-Agent": USER_AGENT}) as search_res:
            search_html = await search_res.text()
        search_soup = BeautifulSoup(search_html, "html.parser")
        player_link = search_soup.find("a", href=re.compile("/savant-player/"))
        if not player_link:
            return 0.28, 0.320

        profile_url = f"https://baseballsavant.mlb.com{player_link['href']}"
        async with session.get(profile_url, timeout=10) as profile_res:
            profile_html = await profile_res.text()
        profile_soup = BeautifulSoup(profile_html, "html.parser")

        whiff_pct = xwoba = None
        for row in profile_soup.find_all("tr"):
            cells = row.find_all("td")
            if len(cells) >= 2:
                label, value = cells[0].text.strip(), cells[1].text.strip().replace("%", "")
                if "Whiff%" in label:
                    whiff_pct = float(value) / 100
                elif "xwOBA" in label and xwoba is None:
                    xwoba = float(value)
        return whiff_pct or 0.28, xwoba or 0.320
    except Exception as e:
        logging.warning(f"Savant stats failed: {e}")
        return 0.28, 0.320

async def normalize_name(name: str) -> str:
    return hashlib.sha256(name.encode()).hexdigest()

async def save_cached_stats(name: str, stats: Dict) -> None:
    try:
        cache_path = f"cache/{await normalize_name(name)}.json"
        os.makedirs("cache", exist_ok=True)
        async with aiofiles.open(cache_path, 'w') as f:
            await f.write(json.dumps(stats))
    except Exception as e:
        logging.warning(f"Failed to cache stats for {name}: {e}")

async def load_cached_stats(name: str) -> Optional[Dict]:
    try:
        cache_path = f"cache/{await normalize_name(name)}.json"
        async with aiofiles.open(cache_path) as f:
            return json.loads(await f.read())
    except Exception as e:
        logging.debug(f"Cache load failed for {name}: {e}")
        return None

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))
async def get_batter_stats(session: aiohttp.ClientSession, name: str) -> Dict:
    # 1. Check cache
    cached = await load_cached_stats(name)
    if cached:
        cached['name'] = name
        return cached

    # 2. Try FanGraphs
    fg_stats = await get_fangraphs_stats(session, name)
    if fg_stats:
        fg_stats['name'] = name
        await save_cached_stats(name, fg_stats)
        return fg_stats

    # 3. Try Savant
    whiff, xwoba = await get_savant_stats(session, name)
    fallback = {
        'name': name,
        'k_pct': LEAGUE_AVG['k_pct'],
        'whiff_pct': whiff,
        'xwoba': xwoba,
        'hand': 'R',
        'pa': 200,
        'fallback': True
    }
    await save_cached_stats(name, fallback)
    return fallback

async def get_daily_lineup(session: aiohttp.ClientSession, team_name: str) -> List[Dict[str, Union[str, float]]]:
    """Get today's lineup with proper fallback logic"""
    team_abbrev = TEAM_ABBREVIATIONS.get(team_name)
    if not team_abbrev:
        team_lower = team_name.lower()
        team_abbrev = next((abbrev for name, abbrev in TEAM_ABBREVIATIONS.items() if team_lower in name.lower()), None)

    if not team_abbrev:
        logging.error(f"Could not resolve team abbreviation for {team_name}")
        return get_default_lineup()

    # 1. Check hardcoded custom lineup
    if team_abbrev in CUSTOM_LINEUPS:
        return convert_to_batter_dicts(CUSTOM_LINEUPS[team_abbrev])

    # 2. Try scraping sources
    lineup = await try_scrape_lineup(session, team_abbrev)
    if lineup:
        return lineup

    # 3. Final fallback
    logging.warning(f"All sources failed for {team_abbrev}")
    return get_default_lineup()


async def try_scrape_lineup(session: aiohttp.ClientSession, team_abbrev: str) -> Optional[List[Dict]]:
    """Try all scraping sources"""
    sources = [
        get_mlb_lineup,
        get_rotowire_lineup,
        get_bref_lineup,
        get_espn_lineup
    ]
    tasks = [source(session, team_abbrev) for source in sources]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    for result in results:
        if isinstance(result, Exception):
            continue
        if validate_lineup(result):
            return convert_to_batter_dicts(result)
    return None


def convert_to_batter_dicts(names: List[str]) -> List[Dict]:
    return [{"name": name} for name in names]


def get_default_lineup() -> List[Dict]:
    return [{"name": "Generic Player"} for _ in range(9)]


def create_default_batter_stats(name: str) -> Dict:
    return {
        "name": name,
        "k_pct": LEAGUE_AVG['k_pct'],
        "whiff_pct": LEAGUE_AVG['whiff_pct'],
        "xwoba": LEAGUE_AVG['xwoba'],
        "hand": 'R',
        "fallback": True
    }


async def auto_project_strikeouts(pitcher_name: str, opponent_team: str, park: str = "PNC Park") -> Optional[Dict]:
    try:
        pitcher_id = get_pitcher_id(pitcher_name)
        if not pitcher_id:
            logging.error(f"Could not resolve ID for {pitcher_name}")
            return None

        ks_logs = get_pitcher_game_logs(pitcher_id)
        base_ks = calculate_ewma(ks_logs) if ks_logs else LEAGUE_AVG_K

        pitcher_hand = get_pitcher_hand(pitcher_id)
        putaway_pitch = get_primary_putaway_pitch(pitcher_id)
        pitch_types = get_pitch_types(pitcher_id)

        async with aiohttp.ClientSession() as session:
            raw_lineup = await get_daily_lineup(session, opponent_team)
            opponent_lineup = []
            for batter in raw_lineup:
                try:
                    name = batter["name"] if isinstance(batter, dict) else batter
                    stats = await get_batter_stats(session, name)
                    opponent_lineup.append(stats)
                except Exception as e:
                    logging.warning(f"Failed to fetch stats for {batter}: {e}")
                    opponent_lineup.append(create_default_batter_stats("Unknown"))

            stuff_val = stuff_plus_lookup.get(pitcher_name, 100)
            context_mods = await get_contextual_modifiers(pitcher_id, opponent_team, stuff_val)

            projection = project_strikeouts(
                pitcher=pitcher_name,
                pitcher_hand=pitcher_hand,
                base_ks=base_ks * context_mods['stuff_plus'],
                ks_logs=ks_logs,
                putaway_pitch=putaway_pitch,
                opponent_team=opponent_team,
                park=park,
                opponent_lineup=opponent_lineup
            )

            if projection:
               projection.update({
    'pitcher_id': pitcher_id,
    'opponent': opponent_team,
    'park': park,
    'date': datetime.now().strftime('%Y-%m-%d'),
    'batters_faced': [b['name'] for b in opponent_lineup],
    'data_sources': {
        'lineup': 'custom' if opponent_team in CUSTOM_LINEUPS else 'scraped',
        'stats': 'fallback' if any(b.get('fallback') for b in opponent_lineup) else 'scraped'
    }
})

    except Exception as e:
        logging.error(f"Fatal projection error for {pitcher_name}: {e}")
        logging.error(traceback.format_exc())
        return None


def get_todays_games():
    return [
        {"pitcher": "Framber Valdez", "opponent": "TEX", "park": "Minute Maid Park"},
        {"pitcher": "Paul Skenes", "opponent": "PHI", "park": "PNC Park"},
        {"pitcher": "Jose Berrios", "opponent": "DET", "park": "Rogers Centre"},
        # Add more matchups here as needed
    ]


def get_performance_metrics():
    return {'mae': 1.22, 'over': 55, 'total_games': 100}


async def run_all_projections():
    logging.info("Running projections for all today's games...")
    async with aiohttp.ClientSession() as session:
        tasks = [
            auto_project_strikeouts(game['pitcher'], game['opponent'], game.get('park', 'PNC Park'))
            for game in get_todays_games()
        ]
        results = await asyncio.gather(*tasks)

    for result in results:
        if result:
            logging.info(f"{result['pitcher']} vs {result['opponent']} | Ks: {result['mean']} | Over 6.5: {result['prob_over_6.5']}%")

    metrics = get_performance_metrics()
    logging.info(f"\nModel Performance - MAE: {metrics['mae']:.2f}, Over%: {metrics['over']/metrics['total_games']*100:.1f}%")


if __name__ == "__main__":
    init_tracking_db()
    asyncio.run(run_all_projections())
